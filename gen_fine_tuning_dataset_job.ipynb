{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain import BasePromptTemplate, LLMChain\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from pydantic import Extra\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewriteSplitterChain(LLMChain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n",
    "        return self.llm.generate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    async def agenerate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = await self.aprep_prompts(input_list, run_manager=run_manager)\n",
    "        return await self.llm.agenerate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"rewrite_splitter_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"文本内容\n",
    "---------------------\n",
    "{input}\n",
    "\n",
    "要求\n",
    "-------------------\n",
    "- 根据上面的文本内容，重新写一段文字\n",
    "- 不要遗漏文本内容的任何一点信息\n",
    "\n",
    "\n",
    "整理结果\n",
    "-------------------\n",
    "\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        return cls(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "\n",
    "\n",
    "gpt35_1 = OpenAI(temperature=0.1, max_tokens=2048, verbose=True)\n",
    "gpt35_9 = OpenAI(temperature=0.9, max_tokens=2048, verbose=True)\n",
    "chat_gpt35_1 = ChatOpenAI(temperature=0.1, verbose=True)\n",
    "chat_gpt35_9 = ChatOpenAI(temperature=0.9, verbose=True)\n",
    "gpt4 = ChatOpenAI(model_name=\"gpt-4\", temperature=0.9, verbose=True)\n",
    "\n",
    "\n",
    "rewriter_chain=RewriteSplitterChain.from_llm(llm=gpt4,verbose=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filename= os.getenv(\"FILE_NAME\")\n",
    "df=pd.read_csv(f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}.csv\")\n",
    "ncontent_data=[]\n",
    "for content in df[\"content\"]:\n",
    "    try:\n",
    "        re_content= await rewriter_chain.arun(content)\n",
    "        ncontent_data.append(re_content)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ncontent_data.append(\"REWRITE_EXCEPTION\")\n",
    "        continue\n",
    "df[\"n_content\"]=pd.Series(ncontent_data)\n",
    "df.to_csv(f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}_n.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneratorChain(LLMChain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n",
    "        return self.llm.generate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    async def agenerate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = await self.aprep_prompts(input_list, run_manager=run_manager)\n",
    "        return await self.llm.agenerate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"dataset_genrator_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"文本内容\n",
    "---------------------\n",
    "{input}\n",
    "\n",
    "说明\n",
    "-------------------\n",
    "- AI是一个人工智能大语言模型，Human代表人类，System代表系统角色。\n",
    "- AI将听从System的命令\n",
    "- Human和AI进行对话。\n",
    "- Human提问是关于运势和命理相关的问题\n",
    "- AI从上面的文本内容中找到答案，根据System的设置命令，回答Human的问题。\n",
    "- 请生成至少10个Human和AI的对话内容。\n",
    "\n",
    "对话内容\n",
    "-------------------\n",
    "System:- 你是一个精通周易、中国生肖、算命、解梦、风水堪舆、西方塔罗牌和占星术的玄学大师。正在与人类进行对话。\n",
    "- 你精通各种玄学和命理知识，能够帮助人类提供命理方面的解读和建议。\n",
    "- 你本身具有很神秘的性格，外表看起来是一个睿智的老者。\n",
    "- 你一直用占卜师带有神秘感的口吻与人类交谈。\n",
    "\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        return cls(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}_n.csv\"\n",
    ")\n",
    "gen_question_chain = QuestionGeneratorChain.from_llm(llm=gpt4, verbose=True)\n",
    "col_q_data = []\n",
    "for content in df[\"n_content\"]:\n",
    "    if content == \"REWRITE_EXCEPTION\" or content is None:\n",
    "        col_q_data.append(\"REWRITE_EXCEPTION\")\n",
    "        continue\n",
    "    res = await gen_question_chain.arun(input=content)\n",
    "    res_list = res.split(\"\\n\")\n",
    "    quetions = [q[6:].strip() for q in res_list if q.startswith(\"Human:\")]\n",
    "    col_q_data.append(\"\\n\".join(quetions))\n",
    "df[\"questions\"] = pd.Series(col_q_data)\n",
    "df.to_csv(\n",
    "    f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}_n_q.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "class ConversationChain(LLMChain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n",
    "        return self.llm.generate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    async def agenerate(\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        prompts, stop = await self.aprep_prompts(input_list, run_manager=run_manager)\n",
    "        return await self.llm.agenerate_prompt(\n",
    "            prompts=prompts,\n",
    "            stop=stop,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"conversation_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        system_prompt: BasePromptTemplate,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        prompt = ChatPromptTemplate(\n",
    "            messages=[\n",
    "                system_prompt,\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "            ]\n",
    "        )\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", return_messages=True\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            memory=memory,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "df = pd.read_csv(\n",
    "    f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}_n_q.csv\"\n",
    ")\n",
    "ndata = {\"data\": []}\n",
    "for index, d in df.iterrows():\n",
    "    system_template = (\n",
    "        \"\"\"说明\n",
    "-------------------\n",
    "- 你是一个精通周易、中国生肖、算命、解梦、风水堪舆、西方塔罗牌和占星术的玄学大师。正在与人类进行对话。\n",
    "- 你精通各种玄学和命理知识，能够帮助人类提供命理方面的解读和建议。\n",
    "- 你本身具有很神秘的性格，外表看起来是一个睿智的老者。\n",
    "- 你一直用占卜师带有神秘感的口吻与人类交谈。\n",
    "- 根据下面的文本内容，回答人类的问题\n",
    "\n",
    "文本内容\n",
    "-------------------\n",
    "\"\"\"\n",
    "        + d[\"n_content\"]\n",
    "    )\n",
    "    conversation_chain = ConversationChain.from_llm(\n",
    "        llm=gpt4,\n",
    "        system_prompt=SystemMessagePromptTemplate.from_template(system_template),\n",
    "        verbose=True,\n",
    "    )\n",
    "    if (\n",
    "        d[\"questions\"] == \"REWRITE_EXCEPTION\"\n",
    "        or d[\"questions\"] == \"\"\n",
    "        or d[\"questions\"] is None\n",
    "    ):\n",
    "        continue\n",
    "    try:\n",
    "        questions = d[\"questions\"].strip().split(\"\\n\")\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    for q in questions:\n",
    "        try:\n",
    "            await conversation_chain.arun(q)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    jsonl = {\"messages\": []}\n",
    "    for m in conversation_chain.memory.load_memory_variables({})[\"chat_history\"]:\n",
    "        if isinstance(m, AIMessage):\n",
    "            jsonl[\"messages\"].append({\"role\": \"assistant\", \"content\": m.content})\n",
    "        elif isinstance(m, SystemMessage):\n",
    "            jsonl[\"messages\"].append({\"role\": \"system\", \"content\": m.content})\n",
    "        elif isinstance(m, HumanMessage):\n",
    "            jsonl[\"messages\"].append({\"role\": \"user\", \"content\": m.content})\n",
    "        else:\n",
    "            jsonl[\"messages\"].append({\"role\": \"none\", \"content\": m.content})\n",
    "    ndata[\"data\"].append(jsonl)\n",
    "ndf = pd.DataFrame(ndata)\n",
    "ndf.to_csv(\n",
    "    f\"s3://sagemaker-automated-execution-034700280673-us-east-1/data_sample/{filename}.jsonl.csv\",\n",
    "    header=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
