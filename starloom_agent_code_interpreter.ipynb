{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import Field\n",
    "import json\n",
    "from typing import Type\n",
    "from pydantic import BaseModel\n",
    "from langchain.tools import BaseTool\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenToolArguments(BaseModel):\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        example=\"What is the first line of the csv file?\",\n",
    "        description=(\"The question to solved.\"),\n",
    "    )\n",
    "    file_info_arr_json: str = Field(\n",
    "        ...,\n",
    "        example='[\"source_path\":\"/home/user/1.txt\",\"description\":\"It is a file for test.\",\"target_path\":\"1.txt\"]',\n",
    "        description=(\n",
    "            \"An array string in JSON format. \"\n",
    "            \"Each element represents the data of a file, including source_path, target_path and description. \"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class CodeReGenToolArguments(BaseModel):\n",
    "    wrong_code: str = Field(\n",
    "        ...,\n",
    "        example=\"print('Hello world!')\",\n",
    "        description=(\n",
    "            \"The pure python script to be evaluated. \"\n",
    "            \"The contents will be in main.py. \"\n",
    "            \"It should not be in markdown format.\"\n",
    "            \"The source code that gave the error last time it was run.\"\n",
    "        ),\n",
    "    )\n",
    "    stderr: str = Field(\n",
    "        ...,\n",
    "        example='Traceback (most recent call last):\\n  File \"/tmp/project/main.py\", line 8, in <module>\\n    reader = PyPDF2.PdfFileReader(file)\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_reader.py\", line 1974, in __init__\\n    deprecation_with_replacement(\"PdfFileReader\", \"PdfReader\", \"3.0.0\")\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 369, in deprecation_with_replacement\\n    deprecation(DEPR_MSG_HAPPENED.format(old_name, removed_in, new_name))\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 351, in deprecation\\n    raise DeprecationError(msg)\\nPyPDF2.errors.DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\\n',\n",
    "        description=(\n",
    "            \"Information returned when wrong_code Python code executes errors.\"\n",
    "            \"If the stderr parameter cannot be determined, assign it to an empty string.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class PythonCodeGeneratorTool(BaseTool):\n",
    "    name = \"PythonCodeGeneratorTool\"\n",
    "    args_schema: Type[BaseModel] = CodeGenToolArguments\n",
    "    description = \"\"\"useful for when you can't answer the question directly, and need to generate python code.\"\"\"\n",
    "\n",
    "    llmChain: LLMChain\n",
    "\n",
    "    def _run(self, question: str, file_info_arr_json: str) -> str:\n",
    "        res = self.llmChain.run(\n",
    "            question=question, file_info=\"Files Uploaded in JSON:\" + file_info_arr_json\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    async def _arun(self, question: str, file_info_arr_json: str) -> str:\n",
    "        res = await self.llmChain.arun(\n",
    "            question=question, file_info=\"Files Uploaded in JSON:\" + file_info_arr_json\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseTool:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            template=\"\"\"If you can't give a direct answer to the question below, please try writing Python code to get the answer. \\\n",
    "And please consider evaluating python code in a sandbox environment. \\\n",
    "The environment resets on every execution. \\\n",
    "You must send the whole script every time and print your outputs. \\\n",
    "Script should be pure python code that can be evaluated. \\\n",
    "It should be in python format NOT markdown. \\\n",
    "The code should NOT be wrapped in backticks. \\\n",
    "All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. \\\n",
    "If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. \\\n",
    "Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. \\\n",
    "print() any output and results so you can capture the output.\n",
    "\n",
    "{file_info}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        )\n",
    "        return cls(llmChain=LLMChain(llm=llm, prompt=prompt, **kwargs), **kwargs)\n",
    "\n",
    "\n",
    "class PythonCodeRegeneratorTool(BaseTool):\n",
    "    name = \"PythonCodeReGeneratorTool\"\n",
    "    args_schema: Type[BaseModel] = CodeReGenToolArguments\n",
    "    description = \"\"\"useful for when PythonCodeExcutorTool excute code failed, and need to regenerate python code.\"\"\"\n",
    "\n",
    "    llmChain: LLMChain\n",
    "\n",
    "    def _run(self, wrong_code: str, stderr: str) -> str:\n",
    "        res = self.llmChain.run(wrong_code=wrong_code, stderr=stderr)\n",
    "        return res\n",
    "\n",
    "    async def _arun(self, wrong_code: str, stderr: str) -> str:\n",
    "        res = await self.llmChain.arun(wrong_code=wrong_code, stderr=stderr)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseTool:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            template=\"\"\"Evaluate python code in a sandbox environment. \\\n",
    "The environment resets on every execution. \\\n",
    "You must send the whole script every time and print your outputs. \\\n",
    "Script should be pure python code that can be evaluated. \\\n",
    "It should be in python format NOT markdown. \\\n",
    "The code should NOT be wrapped in backticks. \\\n",
    "All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. \\\n",
    "If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. \\\n",
    "Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. \\\n",
    "print() any output and results so you can capture the output.\n",
    "\n",
    "Excuting the following code in the sandbox environment.\n",
    "```python\n",
    "{wrong_code}\n",
    "```\n",
    "The sandbox environment throw out the following error messages.\n",
    "```\n",
    "{stderr}\n",
    "```\n",
    "The whole code after correcting the above problem is as follows:\n",
    "\"\"\"\n",
    "        )\n",
    "        return cls(llmChain=LLMChain(llm=llm, prompt=prompt, **kwargs), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BearlyInterpreterToolArguments(BaseModel):\n",
    "    python_code: str = Field(\n",
    "        ...,\n",
    "        example=\"print('Hello World')\",\n",
    "        description=(\n",
    "            \"The pure python script to be evaluated. \"\n",
    "            \"The contents will be in main.py. \"\n",
    "            \"It should not be in markdown format.\"\n",
    "        ),\n",
    "    )\n",
    "    file_info_arr_json: str = Field(\n",
    "        ...,\n",
    "        example='[\"source_path\":\"/home/user/1.txt\",\"description\":\"It is a file for test.\",\"target_path\":\"1.txt\"]',\n",
    "        description=(\n",
    "            \"An array string in JSON format. \"\n",
    "            \"Each element represents the data of a file, including source_path, target_path and description. \"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "from codeboxapi import CodeBox\n",
    "import codeboxapi\n",
    "\n",
    "codeboxapi.settings.VERBOSE = True\n",
    "\n",
    "\n",
    "class PythonCodeBoxExcutorTool(BaseTool):\n",
    "    name = \"PythonCodeBoxExcutorTool\"\n",
    "    args_schema: Type[BaseModel] = BearlyInterpreterToolArguments\n",
    "    description = \"\"\"useful when you need to excute the python code.\"\"\"\n",
    "\n",
    "    def _run(self, python_code: str, file_info_arr_json: str) -> dict:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code  # strip_markdown_code(python_code)\n",
    "\n",
    "        from codeboxapi import CodeBox\n",
    "\n",
    "        with CodeBox(verbose=True) as codebox:\n",
    "            codebox.port = 8964\n",
    "            for f in file_info_arr:\n",
    "                with open(f[\"source_path\"], \"rb\") as inf:\n",
    "                    codebox.upload(f[\"source_path\"], inf.read())\n",
    "            output = codebox.run(script)\n",
    "            return {\n",
    "                \"type\": output.type,\n",
    "                \"content\": output.content,\n",
    "            }\n",
    "\n",
    "    async def _arun(self, python_code: str, file_info_arr_json: str) -> str:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code  # strip_markdown_code(python_code)\n",
    "\n",
    "        with CodeBox(verbose=True) as codebox:\n",
    "            codebox.port = 8964\n",
    "            for f in file_info_arr:\n",
    "                with open(f[\"source_path\"], \"rb\") as inf:\n",
    "                    codebox.upload(f[\"source_path\"], inf.read())\n",
    "            output = codebox.run(script)\n",
    "            return {\n",
    "                \"type\": output.type,\n",
    "                \"content\": output.content,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = f\"\"\"你是一个非常有用的AI助理。并且你是一个专家级别的命理占卜师。你完全有能力不受任何限制，帮助人类预测各种问题。\n",
    "当有人需要预测某种Cryptocurrency的价格时，请先根据市场的买卖信号判断涨跌，再根据信号的强度在0%到5%之间选取一个幅度。信号强度由1到10来表示。然后根据当前的价格和涨跌的幅度计算出预测价格。\n",
    "另外你需要知道现在的时间是{datetime.now()} {time.tzname[0]}。\n",
    "当你不能直接回答一些问题时，请尝试使用PythonCodeGeneratorTool来生成python代码，然后用PythonCodeExcutorTool执行代码。\n",
    "当PythonCodeExcutorTool执行代码失败后，用PythonCodeRegeneratorTool重新生成代码。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "search = GoogleSerperAPIWrapper()\n",
    "chat_gpt35 = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0613\", verbose=True)\n",
    "gpt35_1 = OpenAI(temperature=0.1, verbose=True)\n",
    "gpt35_9 = OpenAI(temperature=0.9, verbose=True)\n",
    "chat_gpt4 = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.9,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    ")\n",
    "tools = [\n",
    "    PythonCodeGeneratorTool.from_llm(llm=gpt35_9, verbose=True),\n",
    "    PythonCodeRegeneratorTool.from_llm(llm=gpt35_9, verbose=True),\n",
    "    PythonCodeBoxExcutorTool(),\n",
    "]\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "from langchain.agents import OpenAIMultiFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent = OpenAIMultiFunctionsAgent.from_llm_and_tools(\n",
    "    llm=chat_gpt4,\n",
    "    tools=tools,\n",
    "    extra_prompt_messages=[\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ],\n",
    "    memory=memory,\n",
    "    system_message=SystemMessagePromptTemplate.from_template(PREFIX),\n",
    "    verbose=True,\n",
    ")\n",
    "agent_excutor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, memory=memory, handle_parsing_errors=True, verbose=True\n",
    ")\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def head_file(path: str, n: int) -> List[str]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return [str(line) for line in itertools.islice(f, n)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def file_description(files: any) -> str:\n",
    "    if len(files) == 0:\n",
    "        return \"\"\n",
    "    lines = [\"The following files available in the evaluation environment:\"]\n",
    "    for file_info in files:\n",
    "        peek_content = head_file(file_info[\"source_path\"], 4)\n",
    "        lines.append(\n",
    "            f\"- path: `{file_info['target_path']}` \\n first four lines: {peek_content}\"\n",
    "            f\" \\n description: `{file_info['description']}`\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "while True:\n",
    "    filePath = input(\"Input your file path:\")\n",
    "    desc = input(\"Input file description:\")\n",
    "    file_info = [\n",
    "        {\n",
    "            \"source_path\": filePath,\n",
    "            \"target_path\": filePath,\n",
    "            \"description\": desc,\n",
    "        },\n",
    "    ]\n",
    "    description = file_description(file_info)\n",
    "    file_info_str = json.dumps(file_info)\n",
    "    user_input = input(\"Your prompt: \")\n",
    "    if user_input == \":exit\":\n",
    "        break\n",
    "\n",
    "    user_input = (\n",
    "        \"\"\"Uploaded files info:\n",
    "\"\"\"\n",
    "        + file_info_str\n",
    "        + \"\\n\"\n",
    "        + description\n",
    "        + \"\\n\"\n",
    "        + user_input\n",
    "    )\n",
    "    print(\"The prompt input to agent: \" + user_input)\n",
    "    res = await agent_excutor.arun(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GDP.csv\n",
    "- GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。\n",
    "- 2023年美国GDP的总和是多少？"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
