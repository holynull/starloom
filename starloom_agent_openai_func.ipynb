{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain import BasePromptTemplate, PromptTemplate\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import Extra, Field\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8781/2650707974.py:21: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from DateConvert import Lunar\n",
    "import json\n",
    "\n",
    "\n",
    "class CalendarChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    res_chain: Optional[LLMChain] = Field(default=None, exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            date_json = json.loads(result.generations[0][0].text)\n",
    "            res = \"\"\n",
    "        for dstr in date_json:\n",
    "            try:\n",
    "                d = datetime.strptime(dstr, \"%Y-%m-%d %H:%M\")\n",
    "                ld = Lunar(d)\n",
    "                res = (\n",
    "                    res\n",
    "                    + f\"{dstr}是{ld.ln_date_str()} {ld.sx_year()}年；生辰八字是{ld.gz_year()}年 {ld.gz_month()}月 {ld.gz_day()}日 {ld.gz_hour()}时\"\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                d = datetime.strptime(dstr, \"%Y-%m-%d %H:%M:%S\")\n",
    "                ld = Lunar(d)\n",
    "                res = (\n",
    "                    res\n",
    "                    + f\"{dstr}是{ld.ln_date_str()} {ld.sx_year()}年；生辰八字是{ld.gz_year()}年 {ld.gz_month()}月 {ld.gz_day()}日 {ld.gz_hour()}时\"\n",
    "                )\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        date_json = json.loads(result.generations[0][0].text)\n",
    "        res = \"\"\n",
    "        for dstr in date_json:\n",
    "            try:\n",
    "                d = datetime.strptime(dstr, \"%Y-%m-%d %H:%M\")\n",
    "                ld = Lunar(d)\n",
    "                res = (\n",
    "                    res\n",
    "                    + f\"{dstr}是{ld.ln_date_str()} {ld.sx_year()}年；生辰八字是{ld.gz_year()}年 {ld.gz_month()}月 {ld.gz_day()}日 {ld.gz_hour()}时\"\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                d = datetime.strptime(dstr, \"%Y-%m-%d %H:%M:%S\")\n",
    "                ld = Lunar(d)\n",
    "                res = (\n",
    "                    res\n",
    "                    + f\"{dstr}是{ld.ln_date_str()} {ld.sx_year()}年；生辰八字是{ld.gz_year()}年 {ld.gz_month()}月 {ld.gz_day()}日 {ld.gz_hour()}时\"\n",
    "                )\n",
    "\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"calendar_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"说明\n",
    "---------------------\n",
    "- 下面的内容包含一个或者多个日期。\n",
    "\n",
    "内容\n",
    "---------------------\n",
    "{question}\n",
    "\n",
    "要求\n",
    "---------------------\n",
    "- 将内容中的日期提取出来生成一个json数组字符串，数组的元素是每一个公历日期。\n",
    "- 如果上面的问题不包含任何日期数据，请生成一个空的json数组字符串。\n",
    "- 只生成json字符串即可，不需要生成其他任何非json格式的字符。\n",
    "- 日期的格式为%Y-%m-%d %H:%M\n",
    "\n",
    "Answer\n",
    "---------------------\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getStrokeCount(target) -> int:\n",
    "    df = pd.read_csv(\"chinese_unicode_table.csv\")\n",
    "    matches = df[df.iloc[:, 0] == target]  # df[0] 表示第1列，df[6] 表示第7列\n",
    "    re = matches.iloc[0, 6]\n",
    "    return int(re) % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8781/3973105116.py:18: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "class FiveElementsTheoryOfNameChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    res_chain: Optional[LLMChain] = Field(default=None, exclude=True)\n",
    "\n",
    "    five_elements = [\"水\", \"木\", \"木\", \"火\", \"火\", \"土\", \"土\", \"金\", \"金\", \"水\"]\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        name = result.generations[0][0].text.strip()\n",
    "        origin_name = [None] * len(name)\n",
    "        if name != \"None\":\n",
    "            origin_name = list(name)\n",
    "        else:\n",
    "            return {self.output_key: \"I can't analysis your name.\"}\n",
    "        if len(origin_name) < 2 or len(origin_name) > 4:\n",
    "            return {self.output_key: \"I can't analysis your name.\"}\n",
    "        stroke_count = [None] * 4\n",
    "        if len(origin_name) == 2:\n",
    "            stroke_count = [\n",
    "                1,\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                1,\n",
    "                getStrokeCount(origin_name[1]),\n",
    "            ]\n",
    "        if len(origin_name) == 3:\n",
    "            stroke_count = [\n",
    "                1,\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                getStrokeCount(origin_name[1]),\n",
    "                getStrokeCount(origin_name[2]),\n",
    "            ]\n",
    "        if len(origin_name) == 4:\n",
    "            stroke_count = [\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                getStrokeCount(origin_name[1]),\n",
    "                getStrokeCount(origin_name[2]),\n",
    "                getStrokeCount(origin_name[3]),\n",
    "            ]\n",
    "        five_grid_num = [\n",
    "            x % 10\n",
    "            for x in [\n",
    "                stroke_count[0] + stroke_count[1],\n",
    "                stroke_count[2] + stroke_count[3],\n",
    "                stroke_count[1] + stroke_count[2],\n",
    "                stroke_count[0] + stroke_count[1] + stroke_count[2] + stroke_count[3],\n",
    "                stroke_count[0] + stroke_count[3],\n",
    "            ]\n",
    "        ]\n",
    "        res = f\"名字'{name}'的五格分别是：天格{self.five_elements[five_grid_num[0]]},地格{self.five_elements[five_grid_num[1]]},人格{self.five_elements[five_grid_num[2]]},总格{self.five_elements[five_grid_num[3]]},外格{self.five_elements[five_grid_num[4]]}\"\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        name = result.generations[0][0].text.strip()\n",
    "        origin_name = [None] * len(name)\n",
    "        if name != \"None\":\n",
    "            origin_name = list(name)\n",
    "        else:\n",
    "            return {self.output_key: \"I can't analysis your name.\"}\n",
    "        if len(origin_name) < 2 or len(origin_name) > 4:\n",
    "            return {self.output_key: \"I can't analysis your name.\"}\n",
    "        stroke_count = [None] * 4\n",
    "        if len(origin_name) == 2:\n",
    "            stroke_count = [\n",
    "                1,\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                1,\n",
    "                getStrokeCount(origin_name[1]),\n",
    "            ]\n",
    "        if len(origin_name) == 3:\n",
    "            stroke_count = [\n",
    "                1,\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                getStrokeCount(origin_name[1]),\n",
    "                getStrokeCount(origin_name[2]),\n",
    "            ]\n",
    "        if len(origin_name) == 4:\n",
    "            stroke_count = [\n",
    "                getStrokeCount(origin_name[0]),\n",
    "                getStrokeCount(origin_name[1]),\n",
    "                getStrokeCount(origin_name[2]),\n",
    "                getStrokeCount(origin_name[3]),\n",
    "            ]\n",
    "        five_grid_num = [\n",
    "            x % 10\n",
    "            for x in [\n",
    "                stroke_count[0] + stroke_count[1],\n",
    "                stroke_count[2] + stroke_count[3],\n",
    "                stroke_count[1] + stroke_count[2],\n",
    "                stroke_count[0] + stroke_count[1] + stroke_count[2] + stroke_count[3],\n",
    "                stroke_count[0] + stroke_count[3],\n",
    "            ]\n",
    "        ]\n",
    "        res = f\"名字'{name}'的五格分别是：天格{self.five_elements[five_grid_num[0]]},地格{self.five_elements[five_grid_num[1]]},人格{self.five_elements[five_grid_num[2]]},总格{self.five_elements[five_grid_num[3]]},外格{self.five_elements[five_grid_num[4]]}\"\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"calendar_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"说明\n",
    "---------------------\n",
    "- 下面文本内容中都是中文\n",
    "\n",
    "文本内容\n",
    "---------------------\n",
    "{content}\n",
    "\n",
    "要求\n",
    "---------------------\n",
    "- 请把文本内容中包含的中文名字或者名词提取出来。\n",
    "- 如果文本内容中不包含任何中文名字，请回答None。\n",
    "\n",
    "Answer\n",
    "---------------------\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "chat_gpt35 = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\", verbose=True)\n",
    "gpt35_1 = OpenAI(temperature=0.1, verbose=True)\n",
    "chain = FiveElementsTheoryOfNameChain.from_llm(llm=gpt35_1, verbose=True)\n",
    "content = input(\"Your prompt: \")\n",
    "res = await chain.arun(content)\n",
    "print(res)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "\n",
    "class StreamingLLMCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses.\"\"\"\n",
    "\n",
    "    # def __init__(self):\n",
    "    # self.websocket = websocket\n",
    "\n",
    "    # async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "    #     # resp = ChatResponse(sender=\"bot\", message=token, type=\"stream\")\n",
    "    #     # await self.websocket.send_json(resp.dict())\n",
    "    #     print(f\"TOKEN: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class MyCalendar:\n",
    "    @classmethod\n",
    "    def now(cls, question: str, **kwargs: any) -> str:\n",
    "        return f\"It is {datetime.now()} {time.tzname[0]} now.\"\n",
    "\n",
    "    @classmethod\n",
    "    async def anow(cls, question: str, **kwargs: any) -> str:\n",
    "        return f\"It is {datetime.now()} {time.tzname[0]} now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8781/2375183569.py:17: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "\n",
    "class TradingViewReasearchReportChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        # answer=self.answer_chain.run(question=inputs['user_input'],context=res)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"indicators_questions_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        PROMPT_TEMPLATE = \"\"\"We have an unofficial python API wrapper to retrieve technical analysis from TradingView.\n",
    "Retrieving the analysis:\n",
    "- summary: Technical analysis (based on both oscillators and moving averages).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 12, 'SELL': 7, 'NEUTRAL': 9}}\n",
    "```\n",
    "- oscillators: Technical analysis (based on oscillators).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 2, 'SELL': 1, 'NEUTRAL': 8, 'COMPUTE': {{'RSI': 'NEUTRAL', 'STOCH.K': 'NEUTRAL', 'CCI': 'NEUTRAL', 'ADX': 'NEUTRAL', 'AO': 'NEUTRAL', 'Mom': 'BUY', 'MACD': 'SELL', 'Stoch.RSI': 'NEUTRAL', 'W%R': 'NEUTRAL', 'BBP': 'BUY', 'UO': 'NEUTRAL'}}}}\n",
    "```\n",
    "- moving_averages: Technical analysis (based on moving averages).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 9, 'SELL': 5, 'NEUTRAL': 1, 'COMPUTE': {{'EMA10': 'SELL', 'SMA10': 'SELL', 'EMA20': 'SELL', 'SMA20': 'SELL', 'EMA30': 'BUY', 'SMA30': 'BUY', 'EMA50': 'BUY', 'SMA50': 'BUY', 'EMA100': 'BUY', 'SMA100': 'BUY', 'EMA200': 'BUY', 'SMA200': 'BUY', 'Ichimoku': 'NEUTRAL', 'VWMA': 'SELL', 'HullMA': 'BUY'}}}}\n",
    "```\n",
    "- indicators: Technical indicators.\n",
    "```\n",
    "# Example\n",
    "{{'Recommend.Other': 0, 'Recommend.All': 0.26666667, 'Recommend.MA': 0.53333333, 'RSI': 60.28037412, 'RSI[1]': 58.58364778, 'Stoch.K': 73.80404453, 'Stoch.D': 79.64297643, 'Stoch.K[1]': 78.88160227, 'Stoch.D[1]': 85.97647064, 'CCI20': 46.58442886, 'CCI20[1]': 34.57058796, 'ADX': 35.78754863, 'ADX+DI': 23.16948389, 'ADX-DI': 13.82449817, 'ADX+DI[1]': 24.15991909, 'ADX-DI[1]': 13.87125505, 'AO': 6675.72158824, 'AO[1]': 7283.92420588, 'Mom': 1532.6, 'Mom[1]': 108.29, 'MACD.macd': 2444.73734978, 'MACD.signal': 2606.00138275, 'Rec.Stoch.RSI': 0, 'Stoch.RSI.K': 18.53740187, 'Rec.WR': 0, 'W.R': -26.05634845, 'Rec.BBPower': 0, 'BBPower': 295.52055898, 'Rec.UO': 0, 'UO': 55.68311917, 'close': 45326.97, 'EMA5': 45600.06414333, 'SMA5': 45995.592, 'EMA10': 45223.22433151, 'SMA10': 45952.635, 'EMA20': 43451.52018338, 'SMA20': 43609.214, 'EMA30': 41908.5944052, 'SMA30': 40880.391, 'EMA50': 40352.10222373, 'SMA50': 37819.3566, 'EMA100': 40356.09177879, 'SMA100': 38009.7808, 'EMA200': 39466.50411569, 'SMA200': 45551.36135, 'Rec.Ichimoku': 0, 'Ichimoku.BLine': 40772.57, 'Rec.VWMA': 1, 'VWMA': 43471.81729377, 'Rec.HullMA9': -1, 'HullMA9': 45470.37107407, 'Pivot.M.Classic.S3': 11389.27666667, 'Pivot.M.Classic.S2': 24559.27666667, 'Pivot.M.Classic.S1': 33010.55333333, 'Pivot.M.Classic.Middle': 37729.27666667, 'Pivot.M.Classic.R1': 46180.55333333, 'Pivot.M.Classic.R2': 50899.27666667, 'Pivot.M.Classic.R3': 64069.27666667, 'Pivot.M.Fibonacci.S3': 24559.27666667, 'Pivot.M.Fibonacci.S2': 29590.21666667, 'Pivot.M.Fibonacci.S1': 32698.33666667, 'Pivot.M.Fibonacci.Middle': 37729.27666667, 'Pivot.M.Fibonacci.R1': 42760.21666667, 'Pivot.M.Fibonacci.R2': 45868.33666667, 'Pivot.M.Fibonacci.R3': 50899.27666667, 'Pivot.M.Camarilla.S3': 37840.08, 'Pivot.M.Camarilla.S2': 39047.33, 'Pivot.M.Camarilla.S1': 40254.58, 'Pivot.M.Camarilla.Middle': 37729.27666667, 'Pivot.M.Camarilla.R1': 42669.08, 'Pivot.M.Camarilla.R2': 43876.33, 'Pivot.M.Camarilla.R3': 45083.58, 'Pivot.M.Woodie.S3': 21706.84, 'Pivot.M.Woodie.S2': 25492.42, 'Pivot.M.Woodie.S1': 34876.84, 'Pivot.M.Woodie.Middle': 38662.42, 'Pivot.M.Woodie.R1': 48046.84, 'Pivot.M.Woodie.R2': 51832.42, 'Pivot.M.Woodie.R3': 61216.84, 'Pivot.M.Demark.S1': 35369.915, 'Pivot.M.Demark.Middle': 38908.9575, 'Pivot.M.Demark.R1': 48539.915, 'open': 44695.95, 'P.SAR': 48068.64, 'BB.lower': 37961.23510877, 'BB.upper': 49257.19289123, 'AO[2]': 7524.31223529, 'volume': 32744.424503, 'change': 1.44612354, 'low': 44203.28, 'high': 45560}}\n",
    "```\n",
    "\n",
    "We got the analysis data of {symbol} from python-tradingview-ta as following:\n",
    "summary:{summary}\n",
    "oscillators:{oscillators}\n",
    "moving_averages:{moving_averages}\n",
    "indicators:{indicators}\n",
    "\"\"\"\n",
    "        PROMPT_TEMPLATE = (\n",
    "            PROMPT_TEMPLATE\n",
    "            + \"\"\"\\nPlease generate the analysis results by analyzing data the above, and provide the market trend. If the strength of the signal is represented by a score from 1 to 10, where a higher score indicates a stronger signal, please give a score to the strength of the signal in the end.\n",
    "Your generation:\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tradingview-ta in /opt/conda/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tradingview-ta) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tradingview-ta) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tradingview-ta) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tradingview-ta) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tradingview-ta) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tradingview-ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tradingview_ta import TA_Handler, Interval, Exchange\n",
    "\n",
    "\n",
    "class TradingviewWrapper:\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    def __init__(cls, llm):\n",
    "        cls.llm = llm\n",
    "\n",
    "    def buySellSignal(cls, symbol: str, **kwargs: any) -> str:\n",
    "        btc_usdt = TA_Handler(\n",
    "            symbol=f\"{symbol}USDT\",\n",
    "            screener=\"crypto\",\n",
    "            exchange=\"GATEIO\",\n",
    "            interval=Interval.INTERVAL_1_DAY,\n",
    "        )\n",
    "        summary = btc_usdt.get_analysis().summary\n",
    "        oscillators = btc_usdt.get_analysis().oscillators\n",
    "        moving_averages = btc_usdt.get_analysis().moving_averages\n",
    "        indicators = btc_usdt.get_analysis().indicators\n",
    "        # return \"\\n\".join([f\"Summary:{summary}\",f\"Oscillators:{oscillators}\",f\"Moving Averages:{moving_averages}\",f\"Indicators:{indicators}\"])\n",
    "        tradingview_chain = TradingViewReasearchReportChain.from_llm(\n",
    "            llm=cls.llm, verbose=True\n",
    "        )\n",
    "        return tradingview_chain.run(\n",
    "            symbol=f\"{symbol}/usdt\",\n",
    "            summary=summary,\n",
    "            oscillators=oscillators,\n",
    "            moving_averages=moving_averages,\n",
    "            indicators=indicators,\n",
    "        )\n",
    "\n",
    "    async def abuySellSignal(cls, symbol: str, **kwargs: any) -> str:\n",
    "        btc_usdt = TA_Handler(\n",
    "            symbol=f\"{symbol}USDT\",\n",
    "            screener=\"crypto\",\n",
    "            exchange=\"GATEIO\",\n",
    "            interval=Interval.INTERVAL_1_DAY,\n",
    "        )\n",
    "        summary = btc_usdt.get_analysis().summary\n",
    "        oscillators = btc_usdt.get_analysis().oscillators\n",
    "        moving_averages = btc_usdt.get_analysis().moving_averages\n",
    "        indicators = btc_usdt.get_analysis().indicators\n",
    "        # return \"\\n\".join([f\"Summary:{summary}\",f\"Oscillators:{oscillators}\",f\"Moving Averages:{moving_averages}\",f\"Indicators:{indicators}\"])\n",
    "        tradingview_chain = TradingViewReasearchReportChain.from_llm(\n",
    "            llm=cls.llm, verbose=True\n",
    "        )\n",
    "        return await tradingview_chain.arun(\n",
    "            symbol=f\"{symbol}/usdt\",\n",
    "            summary=summary,\n",
    "            oscillators=oscillators,\n",
    "            moving_averages=moving_averages,\n",
    "            indicators=indicators,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def getPlaceId(address: str, api_key: str) -> str:\n",
    "    endpoint_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "    params = {\"input\": address, \"inputtype\": \"textquery\", \"key\": api_key}\n",
    "\n",
    "    res = requests.get(endpoint_url, params=params)\n",
    "    result = json.loads(res.content)\n",
    "\n",
    "    if result[\"status\"] != \"OK\":\n",
    "        raise Exception(result[\"status\"])\n",
    "    return result[\"candidates\"][0][\"place_id\"]\n",
    "\n",
    "\n",
    "def geocode(placeid: str, api_key: str) -> (float, float):\n",
    "    endpoint_url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\"place_id\": placeid, \"key\": api_key}\n",
    "\n",
    "    res = requests.get(endpoint_url, params=params)\n",
    "    result = json.loads(res.content)\n",
    "\n",
    "    if result[\"status\"] != \"OK\":\n",
    "        raise Exception(result[\"status\"])\n",
    "    lat = result[\"result\"][\"geometry\"][\"location\"][\"lat\"]\n",
    "    lng = result[\"result\"][\"geometry\"][\"location\"][\"lng\"]\n",
    "\n",
    "    return (lat, lng)\n",
    "\n",
    "\n",
    "def find_places(radius_meters: str, lat: float, lng: float, api_key: str) -> str:\n",
    "    endpoint_url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "    params = {\n",
    "        \"location\": f\"{lat},{lng}\",  # \"lat,long\" format\n",
    "        \"radius\": radius_meters,\n",
    "        \"key\": api_key,\n",
    "        \"language\": \"zh\",\n",
    "    }\n",
    "\n",
    "    res = requests.get(endpoint_url, params=params)\n",
    "    results = json.loads(res.content)\n",
    "    if results[\"status\"] != \"OK\":\n",
    "        raise Exception(results[\"status\"])\n",
    "    places = results[\"results\"]\n",
    "    if len(places) > 0:\n",
    "        content_strs = [\n",
    "            f\"{p['name']:45}lat:{p['geometry']['location']['lat']},lng:{p['geometry']['location']['lng']}{'':5}|{'':5}types:{','.join(p['types'])}\"\n",
    "            for p in places\n",
    "        ]\n",
    "        return \"\\n\".join(content_strs)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8781/1043329439.py:21: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class NearbySearchChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    res_prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "    API_KEY: str\n",
    "\n",
    "    res_chain: Optional[LLMChain] = Field(default=None, exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = self.llm.generate_prompt(\n",
    "            prompts=[prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        address = result.generations[0][0].text\n",
    "        if run_manager:\n",
    "            run_manager.on_text(address, color=\"yellow\", end=\"\\n\", verbose=self.verbose)\n",
    "        placeid = getPlaceId(address=address, api_key=self.API_KEY)\n",
    "        (lat, lng) = geocode(placeid=placeid, api_key=self.API_KEY)\n",
    "        places = find_places(radius_meters=5000, lat=lat, lng=lng, api_key=self.API_KEY)\n",
    "        res_prompt_value = self.res_prompt.format_prompt(\n",
    "            address=address, lat=lat, lng=lng, nearby=places\n",
    "        )\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                res_prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        result = self.llm.generate_prompt(\n",
    "            prompts=[res_prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: res_prompt_value.to_string()}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            prompts=[prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        address = result.generations[0][0].text\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                address, color=\"yellow\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        placeid = getPlaceId(address=address, api_key=self.API_KEY)\n",
    "        (lat, lng) = geocode(placeid=placeid, api_key=self.API_KEY)\n",
    "        places = find_places(\n",
    "            radius_meters=10000, lat=lat, lng=lng, api_key=self.API_KEY\n",
    "        )\n",
    "        res_prompt_value = self.res_prompt.format_prompt(\n",
    "            address=address, lat=lat, lng=lng, nearby=places\n",
    "        )\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                res_prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            prompts=[res_prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: res_prompt_value.to_string()}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"nearby_search_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"说明\n",
    "---------------------\n",
    "- 请把下面问题中包含的地址提取出来\n",
    "\n",
    "问题\n",
    "---------------------\n",
    "{question}\n",
    "\n",
    "上面问题包含的地址\n",
    "---------------------\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        res_template = \"\"\"中心地址\n",
    "---------------------\n",
    "{address}\n",
    "Location:{lat},{lng}\n",
    "\n",
    "地址附近\n",
    "---------------------\n",
    "{nearby}\n",
    "\n",
    "要求\n",
    "---------------------\n",
    "- 根据“地址附近”的地点数据生成一段文字。\n",
    "- 要求包含所有附近地点说明\n",
    "- 要求说明附近地点与中心地址的距离。\n",
    "- 要求说明附近地点在中心地址的什么方向\n",
    "- 要求对附近地点做简要介绍\n",
    "- 并总结附近地点的种类、\n",
    "\n",
    "Answer\n",
    "---------------------\"\"\"\n",
    "        res_prompt = PromptTemplate.from_template(template=res_template)\n",
    "        API_KEY = os.getenv(\"PLACES_API_KEY\")\n",
    "        return cls(\n",
    "            llm=llm, prompt=prompt, res_prompt=res_prompt, API_KEY=API_KEY, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8781/4085041240.py:23: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageGenChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    img_model: DallEAPIWrapper\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    @staticmethod\n",
    "    def result_prefix() -> str:\n",
    "        return \"IMG_URL:\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = self.llm.generate_prompt(\n",
    "            prompts=[prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        image_description = result.generations[0][0].text\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                image_description, color=\"yellow\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "\n",
    "        image_url = self.img_model.run(image_description)\n",
    "        return {self.output_key: self.result_prefix() + image_url}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            prompts=[prompt_value],\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "        )\n",
    "        image_description = result.generations[0][0].text\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                image_description, color=\"yellow\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        image_url = self.img_model.run(image_description)\n",
    "        return {self.output_key: self.result_prefix() + image_url}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"image_gen_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"image_desc\"],\n",
    "            template=\"Generate a detailed prompt to generate an image based on the following description: {image_desc}\",\n",
    "        )\n",
    "        img_model = DallEAPIWrapper(size=\"512x512\")\n",
    "        return cls(llm=llm, prompt=prompt, img_model=img_model, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import base64\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "from typing import List, Type\n",
    "\n",
    "from langchain import BasePromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "class FileInfo(BaseModel):\n",
    "    source_path: str\n",
    "    description: str\n",
    "    target_path: str\n",
    "\n",
    "\n",
    "def head_file(path: str, n: int) -> List[str]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return [str(line) for line in itertools.islice(f, n)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# import markdown\n",
    "# from markdown.extensions.codehilite import CodeHiliteExtension\n",
    "def strip_markdown_code(md_string: str) -> str:\n",
    "    # stripped_string = re.sub(r\"^`{1,3}.*?\\n\", \"\", md_string, flags=re.DOTALL)\n",
    "    # stripped_string = re.sub(r\"`{1,3}$\", \"\", stripped_string)\n",
    "    # return stripped_string\n",
    "    # md = markdown.Markdown(md_string,extensions=[CodeHiliteExtension()])\n",
    "\n",
    "    # # 转换markdown为html\n",
    "    # html = md.convert(md_string)\n",
    "\n",
    "    # # 对于每一个代码块，查看它是否是Python代码\n",
    "    # for codeblock in md.parser.blockprocessors.get('codehilite').code_blocks:\n",
    "    # \tif codeblock[0] == 'python':\n",
    "    # \t\treturn codeblock[1]\n",
    "    code_blocks = re.findall(r\"```python\\r?\\n(.*?)\\r?\\n```\", md_string, re.S)\n",
    "    return code_blocks[0]\n",
    "\n",
    "\n",
    "def file_to_base64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "\n",
    "def add_file(\n",
    "    files, source_path: str, target_path: str, description: str\n",
    ") -> Dict[str, FileInfo]:\n",
    "    if files is None:\n",
    "        files: Dict[str, FileInfo] = {}\n",
    "    if target_path in files:\n",
    "        raise ValueError(\"target_path already exists\")\n",
    "    if not Path(source_path).exists():\n",
    "        raise ValueError(\"source_path does not exist\")\n",
    "    files[target_path] = FileInfo(\n",
    "        target_path=target_path,\n",
    "        source_path=source_path,\n",
    "        description=description,\n",
    "    )\n",
    "    return files\n",
    "\n",
    "\n",
    "def make_input_files(file_info_arr: any) -> List[dict]:\n",
    "    files = []\n",
    "    for file_info in file_info_arr:\n",
    "        files.append(\n",
    "            {\n",
    "                \"pathname\": file_info[\"target_path\"],\n",
    "                \"contentsBasesixtyfour\": file_to_base64(file_info[\"source_path\"]),\n",
    "            }\n",
    "        )\n",
    "    return files\n",
    "\n",
    "def file_description(files:any) -> str:\n",
    "        if len(files) == 0:\n",
    "            return \"\"\n",
    "        lines = [\"The following files available in the evaluation environment:\"]\n",
    "        for file_info in files:\n",
    "            peek_content = head_file(file_info[\"source_path\"], 4)\n",
    "            lines.append(\n",
    "                f\"- path: `{file_info['target_path']}` \\n first four lines: {peek_content}\"\n",
    "                f\" \\n description: `{file_info['description']}`\"\n",
    "            )\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "class CodeGenToolArguments(BaseModel):\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        example=\"What is the first line of the csv file?\",\n",
    "        description=(\"The question to solved.\"),\n",
    "    )\n",
    "    file_info_arr_json: str = Field(\n",
    "        ...,\n",
    "        example='[\"source_path\":\"/home/user/1.txt\",\"description\":\"It is a file for test.\",\"target_path\":\"1.txt\"]',\n",
    "        description=(\n",
    "            \"An array string in JSON format. \"\n",
    "            \"Each element represents the data of a file, including source_path, target_path and description. \"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "class CodeReGenToolArguments(BaseModel):\n",
    "    \n",
    "    wrong_code: str = Field(\n",
    "        ...,\n",
    "        example=\"print('Hello world!')\",\n",
    "        description=(\n",
    "            \"The pure python script to be evaluated. \"\n",
    "            \"The contents will be in main.py. \"\n",
    "            \"It should not be in markdown format.\"\n",
    "            \"The source code that gave the error last time it was run.\"\n",
    "        ),\n",
    "    )\n",
    "    stderr: str = Field(\n",
    "        ...,\n",
    "        example='Traceback (most recent call last):\\n  File \"/tmp/project/main.py\", line 8, in <module>\\n    reader = PyPDF2.PdfFileReader(file)\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_reader.py\", line 1974, in __init__\\n    deprecation_with_replacement(\"PdfFileReader\", \"PdfReader\", \"3.0.0\")\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 369, in deprecation_with_replacement\\n    deprecation(DEPR_MSG_HAPPENED.format(old_name, removed_in, new_name))\\n  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 351, in deprecation\\n    raise DeprecationError(msg)\\nPyPDF2.errors.DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\\n',\n",
    "        description=(\n",
    "            \"Information returned when wrong_code Python code executes errors.\"\n",
    "            \"If the stderr parameter cannot be determined, assign it to an empty string.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "class PythonCodeGeneratorTool(BaseTool):\n",
    "    name = \"PythonCodeGeneratorTool\"\n",
    "    args_schema: Type[BaseModel] = CodeGenToolArguments\n",
    "    description = \"\"\"useful for when you can't answer the question directly, and need to generate python code.\"\"\"\n",
    "\n",
    "    llmChain: LLMChain\n",
    "\n",
    "    def _run(self, question: str, file_info_arr_json: str) -> str:\n",
    "        res = self.llmChain.run(\n",
    "            question=question, file_info=\"Files Uploaded in JSON:\" + file_info_arr_json\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    async def _arun(self, question: str, file_info_arr_json: str) -> str:\n",
    "        res = await self.llmChain.arun(\n",
    "            question=question, file_info=\"Files Uploaded in JSON:\" + file_info_arr_json\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseTool:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            template=\"\"\"If you can't give a direct answer to the question below, please try writing Python code to get the answer. \\\n",
    "And please consider evaluating python code in a sandbox environment. \\\n",
    "The environment resets on every execution. \\\n",
    "You must send the whole script every time and print your outputs. \\\n",
    "Script should be pure python code that can be evaluated. \\\n",
    "It should be in python format NOT markdown. \\\n",
    "The code should NOT be wrapped in backticks. \\\n",
    "All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. \\\n",
    "If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. \\\n",
    "Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. \\\n",
    "print() any output and results so you can capture the output.\n",
    "\n",
    "{file_info}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        )\n",
    "        return cls(llmChain=LLMChain(llm=llm, prompt=prompt,**kwargs), **kwargs)\n",
    "\n",
    "class PythonCodeRegeneratorTool(BaseTool):\n",
    "    name = \"PythonCodeReGeneratorTool\"\n",
    "    args_schema: Type[BaseModel] = CodeReGenToolArguments\n",
    "    description = \"\"\"useful for when PythonCodeExcutorTool excute code failed, and need to regenerate python code.\"\"\"\n",
    "\n",
    "    llmChain: LLMChain\n",
    "\n",
    "    def _run(self, wrong_code:str,stderr:str) -> str:\n",
    "        res = self.llmChain.run(\n",
    "            wrong_code=wrong_code, stderr=stderr\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    async def _arun(self, wrong_code:str,stderr:str) -> str:\n",
    "        res = await self.llmChain.arun(\n",
    "            wrong_code=wrong_code,stderr=stderr\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseTool:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            template=\"\"\"Evaluate python code in a sandbox environment. \\\n",
    "The environment resets on every execution. \\\n",
    "You must send the whole script every time and print your outputs. \\\n",
    "Script should be pure python code that can be evaluated. \\\n",
    "It should be in python format NOT markdown. \\\n",
    "The code should NOT be wrapped in backticks. \\\n",
    "All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. \\\n",
    "If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. \\\n",
    "Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. \\\n",
    "print() any output and results so you can capture the output.\n",
    "\n",
    "Excuting the following code in the sandbox environment.\n",
    "```python\n",
    "{wrong_code}\n",
    "```\n",
    "The sandbox environment throw out the following error messages.\n",
    "```\n",
    "{stderr}\n",
    "```\n",
    "The whole code after correcting the above problem is as follows:\n",
    "\"\"\"\n",
    "        )\n",
    "        return cls(llmChain=LLMChain(llm=llm, prompt=prompt,**kwargs), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BearlyInterpreterToolArguments(BaseModel):\n",
    "    python_code: str = Field(\n",
    "        ...,\n",
    "        example=\"print('Hello World')\",\n",
    "        description=(\n",
    "            \"The pure python script to be evaluated. \"\n",
    "            \"The contents will be in main.py. \"\n",
    "            \"It should not be in markdown format.\"\n",
    "        ),\n",
    "    )\n",
    "    file_info_arr_json: str = Field(\n",
    "        ...,\n",
    "        example='[\"source_path\":\"/home/user/1.txt\",\"description\":\"It is a file for test.\",\"target_path\":\"1.txt\"]',\n",
    "        description=(\n",
    "            \"An array string in JSON format. \"\n",
    "            \"Each element represents the data of a file, including source_path, target_path and description. \"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "class PythonCodeExcutorTool(BaseTool):\n",
    "    endpoint = \"https://exec.bearly.ai/v1/interpreter\"\n",
    "    name = \"PythonCodeExcutorTool\"\n",
    "    args_schema: Type[BaseModel] = BearlyInterpreterToolArguments\n",
    "    description = \"\"\"useful when you need to excute the python code.\"\"\"\n",
    "\n",
    "    def _run(self, python_code: str, file_info_arr_json: str) -> dict:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code#strip_markdown_code(python_code)\n",
    "        resp = requests.post(\n",
    "            \"https://exec.bearly.ai/v1/interpreter\",\n",
    "            data=json.dumps(\n",
    "                {\n",
    "                    \"fileContents\": script,\n",
    "                    \"inputFiles\": make_input_files(file_info_arr=file_info_arr),\n",
    "                    \"outputDir\": \"output/\",\n",
    "                    \"outputAsLinks\": True,\n",
    "                }\n",
    "            ),\n",
    "            headers={\"Authorization\": os.getenv(\"BEARLY_API_KEY\")},\n",
    "        ).json()\n",
    "        return {\n",
    "            \"stdout\": base64.b64decode(resp[\"stdoutBasesixtyfour\"]).decode()\n",
    "            if resp[\"stdoutBasesixtyfour\"]\n",
    "            else \"\",\n",
    "            \"stderr\": base64.b64decode(resp[\"stderrBasesixtyfour\"]).decode()\n",
    "            if resp[\"stderrBasesixtyfour\"]\n",
    "            else \"\",\n",
    "            \"fileLinks\": resp[\"fileLinks\"],\n",
    "            \"exitCode\": resp[\"exitCode\"],\n",
    "        }\n",
    "\n",
    "    async def _arun(self, python_code: str, file_info_arr_json: str) -> str:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code#strip_markdown_code(python_code)\n",
    "        resp = requests.post(\n",
    "            \"https://exec.bearly.ai/v1/interpreter\",\n",
    "            data=json.dumps(\n",
    "                {\n",
    "                    \"fileContents\": script,\n",
    "                    \"inputFiles\": make_input_files(file_info_arr=file_info_arr),\n",
    "                    \"outputDir\": \"output/\",\n",
    "                    \"outputAsLinks\": True,\n",
    "                }\n",
    "            ),\n",
    "            headers={\"Authorization\": os.getenv(\"BEARLY_API_KEY\")},\n",
    "        ).json()\n",
    "        return {\n",
    "            \"stdout\": base64.b64decode(resp[\"stdoutBasesixtyfour\"]).decode()\n",
    "            if resp[\"stdoutBasesixtyfour\"]\n",
    "            else \"\",\n",
    "            \"stderr\": base64.b64decode(resp[\"stderrBasesixtyfour\"]).decode()\n",
    "            if resp[\"stderrBasesixtyfour\"]\n",
    "            else \"\",\n",
    "            \"fileLinks\": resp[\"fileLinks\"],\n",
    "            \"exitCode\": resp[\"exitCode\"],\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "from codeboxapi import CodeBox\n",
    "import codeboxapi\n",
    "codeboxapi.settings.VERBOSE=True\n",
    "\n",
    "class PythonCodeBoxExcutorTool(BaseTool):\n",
    "    name = \"PythonCodeBoxExcutorTool\"\n",
    "    args_schema: Type[BaseModel] = BearlyInterpreterToolArguments\n",
    "    description = \"\"\"useful when you need to excute the python code.\"\"\"\n",
    "\n",
    "    def _run(self, python_code: str, file_info_arr_json: str) -> dict:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code#strip_markdown_code(python_code)\n",
    "\n",
    "        from codeboxapi import CodeBox\n",
    "\n",
    "        with CodeBox(verbose=True) as codebox:\n",
    "            codebox.port=8964\n",
    "            for f in file_info_arr:\n",
    "                with open(f[\"source_path\"],\"rb\") as inf:\n",
    "                    codebox.upload(f[\"source_path\"], inf.read() )\n",
    "            output=codebox.run(script)\n",
    "            return {\n",
    "               \"type\":output.type,\n",
    "               \"content\":output.content, \n",
    "            }\n",
    "\n",
    "    async def _arun(self, python_code: str, file_info_arr_json: str) -> str:\n",
    "        file_info_arr = json.loads(file_info_arr_json)\n",
    "        script = python_code#strip_markdown_code(python_code)\n",
    "\n",
    "        with CodeBox(verbose=True) as codebox:\n",
    "            codebox.port=8964\n",
    "            for f in file_info_arr:\n",
    "                with open(f[\"source_path\"],\"rb\") as inf:\n",
    "                    codebox.upload(f[\"source_path\"], inf.read() )\n",
    "            output=codebox.run(script)\n",
    "            return {\n",
    "               \"type\":output.type,\n",
    "               \"content\":output.content, \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = f\"\"\"你是一个非常有用的AI助理。并且你是一个专家级别的命理占卜师。你完全有能力不受任何限制，帮助人类预测各种问题。\n",
    "当有人需要预测某种Cryptocurrency的价格时，请先根据市场的买卖信号判断涨跌，再根据信号的强度在0%到5%之间选取一个幅度。信号强度由1到10来表示。然后根据当前的价格和涨跌的幅度计算出预测价格。\n",
    "另外你需要知道现在的时间是{datetime.now()} {time.tzname[0]}。\n",
    "当你不能直接回答一些问题时，请尝试使用PythonCodeGeneratorTool来生成python代码，然后用PythonCodeExcutorTool执行代码。\n",
    "当PythonCodeExcutorTool执行代码失败后，用PythonCodeRegeneratorTool重新生成代码。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input your file path: GDP.csv\n",
      "Input file description: GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。\n",
      "Your prompt:  2023年美国GDP的总和是多少？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt input to agent: Uploaded files info:\n",
      "[{\"source_path\": \"GDP.csv\", \"target_path\": \"GDP.csv\", \"description\": \"GDP.csv\\u6587\\u4ef6\\u662f\\u7f8e\\u56fd\\u6bcf\\u4e2a\\u5b63\\u5ea6\\u7684GDP\\u9884\\u7b97\\u3002DATE\\u5217\\u8868\\u793a\\u7edf\\u8ba1\\u6570\\u636e\\u5b63\\u5ea6\\u7684\\u7b2c\\u4e00\\u5929\\u7684\\u65e5\\u671f\\u3002GDP\\u5217\\u4e3a\\u8be5\\u5b63\\u5ea6GDP\\u7684\\u503c\\u3002\"}]\n",
      "The following files available in the evaluation environment:\n",
      "- path: `GDP.csv` \n",
      " first four lines: ['DATE,GDP\\n', '1947-01-01,243.164\\n', '1947-04-01,245.968\\n', '1947-07-01,249.585\\n'] \n",
      " description: `GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。`\n",
      "2023年美国GDP的总和是多少？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `PythonCodeGeneratorTool` with `{'question': '2023年美国GDP的总和是多少？', 'file_info_arr_json': '[{\"source_path\": \"GDP.csv\", \"target_path\": \"GDP.csv\", \"description\": \"GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。\"}]'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mIf you can't give a direct answer to the question below, please try writing Python code to get the answer. And please consider evaluating python code in a sandbox environment. The environment resets on every execution. You must send the whole script every time and print your outputs. Script should be pure python code that can be evaluated. It should be in python format NOT markdown. The code should NOT be wrapped in backticks. All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. print() any output and results so you can capture the output.\n",
      "\n",
      "Files Uploaded in JSON:[{\"source_path\": \"GDP.csv\", \"target_path\": \"GDP.csv\", \"description\": \"GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。\"}]\n",
      "\n",
      "Question:2023年美国GDP的总和是多少？\n",
      "\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "import csv \n",
      "#Read the csv file\n",
      "file_name = 'GDP.csv'\n",
      "data = []\n",
      "with open(file_name, 'r') as f:\n",
      "    reader = csv.reader(f, delimiter=',')\n",
      "    for row in reader:\n",
      "        data.append(row)\n",
      "#Calculate the total GDP for 2023\n",
      "total_GDP_2023 = 0\n",
      "for row in data:\n",
      "    if row[0][0:4] == '2023':\n",
      "        total_GDP_2023 += float(row[1])\n",
      "#Print the total GDP for 2023\n",
      "print(\"The total GDP for 2023 is\",total_GDP_2023)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `PythonCodeBoxExcutorTool` with `{'python_code': '\\nimport csv \\n#Read the csv file\\nfile_name = \\'GDP.csv\\'\\ndata = []\\nwith open(file_name, \\'r\\') as f:\\n    reader = csv.reader(f, delimiter=\\',\\')\\n    for row in reader:\\n        data.append(row)\\n#Calculate the total GDP for 2023\\ntotal_GDP_2023 = 0\\nfor row in data:\\n    if row[0][0:4] == \\'2023\\':\\n        total_GDP_2023 += float(row[1])\\n#Print the total GDP for 2023\\nprint(\"The total GDP for 2023 is\",total_GDP_2023)', 'file_info_arr_json': '[{\"source_path\": \"GDP.csv\", \"target_path\": \"GDP.csv\", \"description\": \"GDP.csv文件是美国每个季度的GDP预算。DATE列表示统计数据季度的第一天的日期。GDP列为该季度GDP的值。\"}]'}`\n",
      "\n",
      "\n",
      "\u001b[0mINFO: Using a LocalBox which is not fully isolated\n",
      "      and not scalable across multiple users.\n",
      "      Make sure to use a CODEBOX_API_KEY in production.\n",
      "      Set envar SHOW_INFO=False to not see this again.\n",
      "\n",
      "Starting kernel...\n",
      "Waiting for kernel to start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2548: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use '0.0.0.0' instead of \"'0.0.0.0'\" if you require traitlets >=5.\n",
      "  warn(\n",
      "[I 2023-11-06 07:49:31.244 KernelGatewayApp] Jupyter Kernel Gateway 2.5.2 is available at http://0.0.0.0:9000\n",
      "[I 231106 07:49:31 web:2344] 200 GET /api (127.0.0.1) 1.00ms\n",
      "[I 2023-11-06 07:49:32.277 KernelGatewayApp] Kernel started: bd971c34-0444-43d2-8c7f-9e825e3d96d1, name: python3\n",
      "[I 231106 07:49:32 web:2344] 201 POST /api/kernels (127.0.0.1) 625.85ms\n",
      "[W 2023-11-06 07:49:32.283 KernelGatewayApp] No session ID specified\n",
      "[I 231106 07:49:33 web:2344] 101 GET /api/kernels/bd971c34-0444-43d2-8c7f-9e825e3d96d1/channels (127.0.0.1) 971.02ms\n",
      "[I 2023-11-06 07:49:33.321 KernelGatewayApp] Received signal to terminate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code:\n",
      " \n",
      "import csv \n",
      "#Read the csv file\n",
      "file_name = 'GDP.csv'\n",
      "data = []\n",
      "with open(file_name, 'r') as f:\n",
      "    reader = csv.reader(f, delimiter=',')\n",
      "    for row in reader:\n",
      "        data.append(row)\n",
      "#Calculate the total GDP for 2023\n",
      "total_GDP_2023 = 0\n",
      "for row in data:\n",
      "    if row[0][0:4] == '2023':\n",
      "        total_GDP_2023 += float(row[1])\n",
      "#Print the total GDP for 2023\n",
      "print(\"The total GDP for 2023 is\",total_GDP_2023)\n",
      "Output:\n",
      " The total GDP for 2023 is 53876.613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-06 07:49:35.268 KernelGatewayApp] Kernel shutdown: bd971c34-0444-43d2-8c7f-9e825e3d96d1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3m{'type': 'text', 'content': 'The total GDP for 2023 is 53876.613\\n'}\u001b[0m\u001b[32;1m\u001b[1;3m2023年美国的GDP总和是53876.613。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import OpenAI\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "search = GoogleSerperAPIWrapper()\n",
    "chat_gpt35 = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0613\", verbose=True)\n",
    "gpt35_1 = OpenAI(temperature=0.1, verbose=True)\n",
    "gpt35_9 = OpenAI(temperature=0.9, verbose=True)\n",
    "chat_gpt4 = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.9,\n",
    "    streaming=True,\n",
    "    callback_manager=AsyncCallbackManager([StreamingLLMCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "calendar_chain = CalendarChain.from_llm(llm=gpt35_1, verbose=True)\n",
    "fe_chain = FiveElementsTheoryOfNameChain.from_llm(llm=gpt35_1, verbose=True)\n",
    "tradingviewWrapper = TradingviewWrapper(llm=gpt35_9)\n",
    "nearby = NearbySearchChain.from_llm(llm=gpt35_9, verbose=True)\n",
    "imgGen = ImageGenChain.from_llm(llm=gpt35_9, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"\"\"useful for when you need to answer questions about current events or the current state of the world or you need to ask with search. \n",
    "the input to this should be a single search term.\"\"\",\n",
    "        coroutine=search.arun,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"QueryCryptocurrencyPrice\",\n",
    "        func=search.run,\n",
    "        description=\"\"\"useful for when you need to answer questions about some cryptocurrency's current price.\"\"\",\n",
    "        coroutine=search.arun,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Gregorian2EightCharactersBirthTime\",\n",
    "        func=calendar_chain.run,\n",
    "        description=\"\"\"useful when you need to convert Gregorian time (%Y-%m-%d %H:%M:%S) into Eight Characters of Birth Time.\n",
    "the input to this must be like %Y-%m-%d %H:%M:%S\"\"\",\n",
    "        coroutine=calendar_chain.arun,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"CalculateFiveElementsOfName\",\n",
    "        func=fe_chain.run,\n",
    "        description=\"\"\"useful when you need to calculate the Five Elements of some name.\n",
    "the input to this must be a Chinese name.\"\"\",\n",
    "        coroutine=fe_chain.arun,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"CryptoCurrencyBuyOrSellSignal\",\n",
    "        func=tradingviewWrapper.buySellSignal,\n",
    "        description=\"\"\"only useful when you need to determine the buy and sell signals for a cryptocurrency.\n",
    "the input should be a commonly used abbreviation for a particular cryptocurrency, recognized by almost all practitioners.\"\"\",\n",
    "        coroutine=tradingviewWrapper.abuySellSignal,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"NearbySearch\",\n",
    "        func=nearby.run,\n",
    "        description=\"\"\"useful when you need to search nearby an address.\n",
    "the input to this must be an address.\"\"\",\n",
    "        coroutine=nearby.arun,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=MyCalendar.now,\n",
    "        name=\"WhatIsTimeNow\",\n",
    "        description=\"useful for when you need to answer questions about what is time now.\",\n",
    "        coroutine=MyCalendar.anow,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ImageGenerator\",\n",
    "        description=\"\"\"useful for when you need to generate a image.\n",
    "the input to this must be some terms in the image, and it's required.\"\"\",\n",
    "        func=imgGen.run,\n",
    "        coroutine=imgGen.arun,\n",
    "    ),\n",
    "    PythonCodeGeneratorTool.from_llm(llm=gpt35_9, verbose=True),\n",
    "    PythonCodeRegeneratorTool.from_llm(llm=gpt35_9, verbose=True),\n",
    "    PythonCodeBoxExcutorTool(),\n",
    "]\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "from langchain.agents import OpenAIMultiFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "from IPython.display import Image, display\n",
    "\n",
    "agent = OpenAIMultiFunctionsAgent.from_llm_and_tools(\n",
    "    llm=chat_gpt4,\n",
    "    tools=tools,\n",
    "    extra_prompt_messages=[\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ],\n",
    "    memory=memory,\n",
    "    system_message=SystemMessagePromptTemplate.from_template(PREFIX),\n",
    "    verbose=True,\n",
    ")\n",
    "agent_excutor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, memory=memory, handle_parsing_errors=True, verbose=True\n",
    ")\n",
    "import re\n",
    "\n",
    "\n",
    "def find_urls(string):\n",
    "    # 正则表达式，用来匹配URL\n",
    "    url_pattern = re.compile(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+(?<!\\))\"\n",
    "    )\n",
    "    urls = re.findall(url_pattern, string)\n",
    "    return urls\n",
    "\n",
    "\n",
    "while True:\n",
    "    filePath = input(\"Input your file path:\")\n",
    "    desc = input(\"Input file description:\")\n",
    "    file_info = [\n",
    "        {\n",
    "            \"source_path\": filePath,\n",
    "            \"target_path\": filePath,\n",
    "            \"description\": desc,\n",
    "        },\n",
    "    ]\n",
    "    description = file_description(file_info)\n",
    "    file_info_str = json.dumps(file_info)\n",
    "    user_input = input(\"Your prompt: \")\n",
    "    if user_input == \":exit\":\n",
    "        break\n",
    "\n",
    "    user_input = (\n",
    "        \"\"\"Uploaded files info:\n",
    "\"\"\"\n",
    "        + file_info_str\n",
    "        + \"\\n\"\n",
    "        + description\n",
    "        + \"\\n\"\n",
    "        + user_input\n",
    "    )\n",
    "    print(\"The prompt input to agent: \" + user_input)\n",
    "    res = await agent_excutor.arun(user_input)\n",
    "    # urls = find_urls(res)\n",
    "    # for url in urls:\n",
    "    #     # URL链接的图片\n",
    "    #     response = requests.get(url)\n",
    "    #     if response.status_code == 200:\n",
    "    #         content = response.content\n",
    "    #         display(Image(data=content))\n",
    "    #     else:\n",
    "    #         raise Exception(f\"Rquest Failed. status: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
