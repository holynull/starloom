{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain import BasePromptTemplate, PromptTemplate\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import Extra, Field\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class FileInfo(BaseModel):\n",
    "    source_path: str\n",
    "    description: str\n",
    "    target_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import PyPDF2\n",
      "\n",
      "def extract_text_from_pdf(file_path):\n",
      "    pdf_file_obj = open(file_path, 'rb')\n",
      "    pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n",
      "    page_obj = pdf_reader.getPage(0)\n",
      "    text = page_obj.extractText()\n",
      "    pdf_file_obj.close()\n",
      "    return text\n",
      "\n",
      "print(extract_text_from_pdf('swftc_analysis.pdf'))\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def head_file(path: str, n: int) -> List[str]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return [str(line) for line in itertools.islice(f, n)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# import markdown\n",
    "# from markdown.extensions.codehilite import CodeHiliteExtension\n",
    "def strip_markdown_code(md_string: str) -> str:\n",
    "    # stripped_string = re.sub(r\"^`{1,3}.*?\\n\", \"\", md_string, flags=re.DOTALL)\n",
    "    # stripped_string = re.sub(r\"`{1,3}$\", \"\", stripped_string)\n",
    "    # return stripped_string\n",
    "    # md = markdown.Markdown(md_string,extensions=[CodeHiliteExtension()])\n",
    "\n",
    "    # # 转换markdown为html\n",
    "    # html = md.convert(md_string)\n",
    "\n",
    "    # # 对于每一个代码块，查看它是否是Python代码\n",
    "    # for codeblock in md.parser.blockprocessors.get('codehilite').code_blocks:\n",
    "    # \tif codeblock[0] == 'python':\n",
    "    # \t\treturn codeblock[1]\t\n",
    "    code_blocks = re.findall(r'```python\\r?\\n(.*?)\\r?\\n```', md_string, re.S)\n",
    "    return \"\\n\".join(code_blocks)\n",
    "\n",
    "def file_to_base64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "\n",
    "def add_file(\n",
    "    files, source_path: str, target_path: str, description: str\n",
    ") -> Dict[str, FileInfo]:\n",
    "    if files is None:\n",
    "        files: Dict[str, FileInfo] = {}\n",
    "    if target_path in files:\n",
    "        raise ValueError(\"target_path already exists\")\n",
    "    if not Path(source_path).exists():\n",
    "        raise ValueError(\"source_path does not exist\")\n",
    "    files[target_path] = FileInfo(\n",
    "        target_path=target_path,\n",
    "        source_path=source_path,\n",
    "        description=description,\n",
    "    )\n",
    "    return files\n",
    "\n",
    "\n",
    "def make_input_files(sfiles: Dict[str, FileInfo]) -> List[dict]:\n",
    "    files = []\n",
    "    for target_path, file_info in sfiles.items():\n",
    "        files.append(\n",
    "            {\n",
    "                \"pathname\": target_path,\n",
    "                \"contentsBasesixtyfour\": file_to_base64(file_info.source_path),\n",
    "            }\n",
    "        )\n",
    "    return files\n",
    "\n",
    "s=\"\"\"对不起，我无法直接阅读PDF文件的内容。我需要使用Python的PDF处理库，如PyPDF2，来提取PDF文件的文本。然而，这种方法可能无法完全准确地提取所有的文本，特别是如果文本是图像或者有复杂的格式。这是一个可能的代码示例：\n",
    "\n",
    "```python\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pdf_file_obj = open(file_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n",
    "    page_obj = pdf_reader.getPage(0)\n",
    "    text = page_obj.extractText()\n",
    "    pdf_file_obj.close()\n",
    "    return text\n",
    "\n",
    "print(extract_text_from_pdf('swftc_analysis.pdf'))\n",
    "```\n",
    "\n",
    "这段代码将打开PDF文件，创建一个PDF阅读器对象，获取第一页，提取文本，然后关闭文件。最后，它将打印出提取的文本。\"\"\"\n",
    "print(strip_markdown_code(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2258/802105736.py:21: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.3/migration/\n",
      "  extra = Extra.forbid\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "class BearlyChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    api_key: str\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    res_chain: Optional[LLMChain] = Field(default=None, exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        script = strip_markdown_code(result.generations[0][0].text)\n",
    "        resp = requests.post(\n",
    "            \"https://exec.bearly.ai/v1/interpreter\",\n",
    "            data=json.dumps(\n",
    "                {\n",
    "                    \"fileContents\": script,\n",
    "                    \"inputFiles\": make_input_files(inputs[\"file_info\"]),\n",
    "                    \"outputDir\": \"output/\",\n",
    "                    \"outputAsLinks\": True,\n",
    "                }\n",
    "            ),\n",
    "            headers={\"Authorization\": self.api_key},\n",
    "        ).json()\n",
    "        res = (\n",
    "            \"stdout:\" + base64.b64decode(resp[\"stdoutBasesixtyfour\"]).decode()\n",
    "            if resp[\"stdoutBasesixtyfour\"]\n",
    "            else \"\"\n",
    "            + \"\\n\"\n",
    "            + \"stderr:\"\n",
    "            + base64.b64decode(resp[\"stderrBasesixtyfour\"]).decode()\n",
    "            if resp[\"stderrBasesixtyfour\"]\n",
    "            else \"\"\n",
    "            + \"\\n\"\n",
    "            + \"fileLinks:\"\n",
    "            + resp[\"fileLinks\"]\n",
    "            + \"\\n\"\n",
    "            + \"exitCode:\"\n",
    "            + resp[\"exitCode\"]\n",
    "        )\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        result = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                result.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        script = strip_markdown_code(result.generations[0][0].text)\n",
    "        resp = requests.post(\n",
    "            \"https://exec.bearly.ai/v1/interpreter\",\n",
    "            data=json.dumps(\n",
    "                {\n",
    "                    \"fileContents\": script,\n",
    "                    \"inputFiles\": make_input_files(inputs[\"file_info\"]),\n",
    "                    \"outputDir\": \"output/\",\n",
    "                    \"outputAsLinks\": True,\n",
    "                }\n",
    "            ),\n",
    "            headers={\"Authorization\": self.api_key},\n",
    "        ).json()\n",
    "        res = (\n",
    "            \"stdout:\" + base64.b64decode(resp[\"stdoutBasesixtyfour\"]).decode()\n",
    "            if resp[\"stdoutBasesixtyfour\"]\n",
    "            else \"\"\n",
    "            + \"\\n\"\n",
    "            + \"stderr:\"\n",
    "            + base64.b64decode(resp[\"stderrBasesixtyfour\"]).decode()\n",
    "            if resp[\"stderrBasesixtyfour\"]\n",
    "            else \"\"\n",
    "            + \"\\n\"\n",
    "            + \"fileLinks:\"\n",
    "            + resp[\"fileLinks\"]\n",
    "            + \"\\n\"\n",
    "            + \"exitCode:\"\n",
    "            + resp[\"exitCode\"]\n",
    "        )\n",
    "        return {self.output_key: res}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"bearly_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def file_description(self, files: Dict[str, FileInfo]) -> str:\n",
    "        if len(files) == 0:\n",
    "            return \"\"\n",
    "        lines = [\"The following files available in the evaluation environment:\"]\n",
    "        for target_path, file_info in files.items():\n",
    "            peek_content = head_file(file_info.source_path, 4)\n",
    "            lines.append(\n",
    "                f\"- path: `{target_path}` \\n first four lines: {peek_content}\"\n",
    "                f\" \\n description: `{file_info.description}`\"\n",
    "            )\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        api_key: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> Chain:\n",
    "        template = \"\"\"If you can't give a direct answer to the question below, please try writing Python code to get the answer. \\\n",
    "And please consider evaluating python code in a sandbox environment. \\\n",
    "The environment resets on every execution. \\\n",
    "You must send the whole script every time and print your outputs. \\\n",
    "Script should be pure python code that can be evaluated. \\\n",
    "It should be in python format NOT markdown. \\\n",
    "The code should NOT be wrapped in backticks. \\\n",
    "All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. \\\n",
    "If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. \\\n",
    "Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. \\\n",
    "print() any output and results so you can capture the output.\n",
    "{file_description}\n",
    "\n",
    "{file_info}\n",
    "\n",
    "Question:{question}\n",
    "Answer:\"\"\"\n",
    "        prompt = PromptTemplate.from_template(template=template)\n",
    "        return cls(llm=llm, prompt=prompt, api_key=api_key, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input your file path: swftc_analysis.pdf\n",
      "Input your quetion: 请提取一下上面文件第一页的内容。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new BearlyChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIf you can't give a direct answer to the question below, please try writing Python code to get the answer. And please consider evaluating python code in a sandbox environment. The environment resets on every execution. You must send the whole script every time and print your outputs. Script should be pure python code that can be evaluated. It should be in python format NOT markdown. The code should NOT be wrapped in backticks. All python packages including requests, matplotlib, scipy, numpy, pandas, etc are available. If you have any files outputted write them to \"output/\" relative to the execution path. Output can only be read from the directory, stdout, and stdin. Do not use things like plot.show() as it will not work instead write them out `output/` and a link to the file will be returned. print() any output and results so you can capture the output.\n",
      "The following files available in the evaluation environment:\n",
      "- path: `swftc_analysis.pdf` \n",
      " first four lines: [] \n",
      " description: ``\n",
      "\n",
      "{'swftc_analysis.pdf': FileInfo(source_path='swftc_analysis.pdf', description='', target_path='swftc_analysis.pdf')}\n",
      "\n",
      "Question:请提取一下上面文件第一页的内容。\n",
      "Answer:\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m对不起，我不能直接回答你的问题，但我可以写一段Python代码来解决这个问题。我们可以使用PyPDF2库来读取PDF文件的内容。\n",
      "\n",
      "```python\n",
      "import PyPDF2\n",
      "\n",
      "# Open the PDF file in read-binary mode\n",
      "pdf_file = open('swftc_analysis.pdf', 'rb')\n",
      "\n",
      "# Create a PDF file reader object\n",
      "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
      "\n",
      "# Get the text from the first page\n",
      "page_obj = pdf_reader.getPage(0)\n",
      "text = page_obj.extractText()\n",
      "\n",
      "# Close the PDF file\n",
      "pdf_file.close()\n",
      "\n",
      "# Print the text from the first page\n",
      "print(text)\n",
      "```\n",
      "\n",
      "这段代码将打开PDF文件，创建一个PDF文件阅读器对象，从第一页获取文本，然后关闭PDF文件。最后，它将打印出第一页的文本。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:\n",
      "stderr:Traceback (most recent call last):\n",
      "  File \"/tmp/project/main.py\", line 7, in <module>\n",
      "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
      "  File \"/venv/lib/python3.10/site-packages/PyPDF2/_reader.py\", line 1974, in __init__\n",
      "    deprecation_with_replacement(\"PdfFileReader\", \"PdfReader\", \"3.0.0\")\n",
      "  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 369, in deprecation_with_replacement\n",
      "    deprecation(DEPR_MSG_HAPPENED.format(old_name, removed_in, new_name))\n",
      "  File \"/venv/lib/python3.10/site-packages/PyPDF2/_utils.py\", line 351, in deprecation\n",
      "    raise DeprecationError(msg)\n",
      "PyPDF2.errors.DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "chain = BearlyChain.from_llm(llm=llm, api_key=os.getenv(\"BEARLY_API_KEY\"), verbose=True)\n",
    "filePath = input(\"Input your file path:\")\n",
    "files = add_file(None, source_path=filePath, target_path=filePath, description=\"\")\n",
    "file_description = BearlyChain.file_description(files=files)\n",
    "quesiton = input(\"Input your quetion:\")\n",
    "res = await chain.arun(\n",
    "    question=quesiton, file_description=file_description, file_info=files\n",
    ")\n",
    "print(f\"AI:{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
